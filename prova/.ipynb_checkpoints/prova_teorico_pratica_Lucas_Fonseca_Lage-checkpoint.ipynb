{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lgyh5vuO-eF5"
   },
   "source": [
    "# Fundamentos de Linguística Computacional\n",
    "*Semestre: 2021/1*\n",
    "\n",
    "*Professores: Adriana Pagano, Evandro L. T. P. Cunha, Thiago Castro Ferreira*\n",
    "\n",
    "**Prova teórico-prática**\n",
    "\n",
    "Instruções para a realização desta prova:\n",
    "1. copie este notebook Colab para o seu Google Drive e resolva as questões na sua cópia;\n",
    "2. resolva cada questão na célula indicada, sem alterar a ordem das células. Cada questão tem o valor de 5 pontos;\n",
    "3. quando a questão exigir a impressão de resultados, procure ser claro na descrição das saídas. Em outras palavras: em vez de apenas imprimir, digamos, o número \"4\", imprima \"Número de caracteres da palavra 'casa' = 4\". As impressões, seja no Colab, seja em arquivos separados, devem ser claras, autocontidas e autoexplicativas;\n",
    "4. todas as instruções contendo verbos como \"contar\", \"calcular\" etc. pressupõem que as tarefas serão realizadas por meio de código, não manualmente;\n",
    "5. quando a questão informar que se exige uma análise qualitativa (única situação em que não se pressupõe a elaboração de códigos), use o comentário do Python (#) para respondê-la;\n",
    "6. envie o link do seu notebook Colab por mensagem privada no Teams a um dos professores. Não se esqueça de liberar o seu Colab para que possa ser acessado por qualquer pessoa que possua o link.\n",
    "\n",
    "*Valor da prova: 30 pontos  \n",
    "Data-limite para entrega: 14/07/2021, às 13h59*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bf6LVqLQqLWS"
   },
   "source": [
    "*As questões a seguir se basearão em um par de textos/corpora à sua escolha. Primeiro, crie uma pasta com seu nome no diretório:\n",
    "https://drive.google.com/drive/folders/1l6rqBgnnj3puZ9zHQEjg6RH9M6UuElS3?usp=sharing  \n",
    "Insira os dois textos/corpora lá, para que possamos rodar seu códigos usando o mesmo material que você usou.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ZP1hOom6-Zo9"
   },
   "outputs": [],
   "source": [
    "# Cabeçalho para inicialização. Execute este cabeçalho, como de costume, para a resolução das questões.\n",
    "\n",
    "import nltk\n",
    "# nltk.download('all')\n",
    "# from nltk.book import *\n",
    "from nltk import FreqDist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IyiTT0o6ECM6"
   },
   "source": [
    "**Questão 1** \n",
    "(a) Importe seus dois textos/corpora para o diretório '/content/prova-flc' usando o \"PlaintextCorpusReader\".  \n",
    "(b) Calcule o número de types e de tokens em cada um dos seus textos/corpora.  \n",
    "(c) Crie uma função em Python que calcula o índice de riqueza/diversidade lexical (razão type-token); rode essa função para cada um dos seus textos/corpora.  \n",
    "(d) Imprima as saídas dos itens (b) e (c) no arquivo '/content/questao1.txt'.  \n",
    "(e) Você observou grande diferença entre os índices de riqueza lexical encontrados? Na sua opinião, por quê? _[análise qualitativa]_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MyhEgB_KomV2",
    "outputId": "df013fa6-9f7d-4b1c-e2ee-1a1206456711"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "01_Welcome_to_Republic_City\n",
      "Número de Tokens: 12308\n",
      "Número de types: 2049\n",
      "\n",
      "50_The_Last_Stand_\n",
      "Número de Tokens: 7961\n",
      "Número de types: 1635\n",
      "\n",
      "01_Welcome_to_Republic_City\n",
      "Diversidade Lexical: 0.166\n",
      "\n",
      "50_The_Last_Stand_\n",
      "Diversidade Lexical: 0.205\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def corpus_freqs(word_list):\n",
    "        '''Função para retornar a diversidade, recebe uma lista de palavras.\n",
    "        retorna o valor da diversidade lexical como float\n",
    "        '''\n",
    "        return(len(set(word_list))/len(word_list))\n",
    "    \n",
    "# Resolução da questão 1.\n",
    "\n",
    "# a \n",
    "\n",
    "corpus = nltk.corpus.PlaintextCorpusReader('./content/prova-flc', '.*')\n",
    "\n",
    "with open('./content/questao1.txt', 'w') as questao:\n",
    "\n",
    "# b\n",
    "    for file in corpus.fileids():\n",
    "        words = corpus.words(file)\n",
    "        questao.write(('\\n' + file + '\\n'))\n",
    "        questao.write('Número de Tokens: {0}\\nNúmero de types: {1}\\n'.format(len(words),len(set(words))))\n",
    "    \n",
    "# c\n",
    "    \n",
    "    for file in corpus.fileids():\n",
    "        questao.write('\\n' + file)\n",
    "        questao.write('\\nDiversidade Lexical: {0:.3}\\n'.format(corpus_freqs(corpus.words(file))))\n",
    "\n",
    "# d                  \n",
    "with open('./content/questao1.txt','r') as resposta:\n",
    "    print(resposta.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RESPONDER AINDA A QUESTÃO 1 E"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SKjIsEj4E-DE"
   },
   "source": [
    "## Questão 2\n",
    "(a) Imprima os 50 types mais frequentes em cada um dos seus textos/corpora.  \n",
    "(b) Elabore um programa que retorne, dentre os 50 types mais frequentes de cada texto/corpus: (i) uma lista dos types comuns a ambos (ou seja, dos que estão entre os 50 mais frequentes em ambos); (ii) uma lista dos types exclusivos do texto/corpus 1; (iii) uma lista dos types exclusivos do texto/corpus 2.  \n",
    "(c) Imprima as saídas dos itens (a) e (b) no arquivo '/content/questao2.txt'.  \n",
    "(d) O que se pode observar a partir dos resultados obtidos no item (b)? Quais tipos de types são frequentes em ambos os textos/corpora e quais são exclusivos de cada um? O que se pode concluir disso? _[análise qualitativa]_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "5jd6Vk90DKwL"
   },
   "outputs": [],
   "source": [
    "def freqs_groups(words_list):\n",
    "    '''recebe uma lista de mais frequentes da função FreqDist. Retorna uma tupla com três sets.\n",
    "    O primeiro set contém os types que ocorrem em ambos os arquivos.\n",
    "    O segundo set contém os types que ocorrem no conjunto a, mas não no conjunto b\n",
    "    O segundo set contém os types que ocorrem no conjunto a, mas não no conjunto b\n",
    "    '''\n",
    "    list_of_words = []\n",
    "    for freqs in words_list:\n",
    "        list_of_words.append([i[0] for i in freqs])\n",
    "    a = set(list_of_words[0])\n",
    "    b = set(list_of_words[1])\n",
    "    return(a.intersection(b), a.difference(b), b.difference(a))\n",
    "\n",
    "\n",
    "\n",
    "# Resolução da questão 2.\n",
    "list_of_freqs = []\n",
    "with open('./content/questao2','w') as questao:\n",
    "# a\n",
    "    for file in corpus.fileids():\n",
    "        most_freqs = nltk.FreqDist(corpus.words(file)).most_common(50)\n",
    "        questao.write('\\n' + file + '\\n')\n",
    "        for t in most_freqs:\n",
    "            questao.write('token: {0}\\t\\t freq: {1}\\n'.format(str(t[0]),str(t[1])))\n",
    "        list_of_freqs.append(most_freqs)\n",
    "    questao.write('\\n\\n\\n\\n\\n')\n",
    "    \n",
    "    \n",
    "# b\n",
    "    operacao_conjunto = freqs_groups(list_of_freqs)\n",
    "    questao.write('2. b)')\n",
    "    questao.write('\\nTypes comuns a ambos os textos:\\n{0}'.format(' '.join(operacao_conjunto[0])))\n",
    "    questao.write('\\nTypes exclusivos do texto 1:\\n{0}'.format(' '.join(operacao_conjunto[1])))\n",
    "    questao.write('\\nTypes exclusivos do texto 2:\\n{0}'.format(' '.join(operacao_conjunto[2])))\n",
    "\n",
    "# c    \n",
    "with open('./content/questao2','r') as resposta:\n",
    "    print(resposta.read())\n",
    "    \n",
    "##### Questão 2. e)\n",
    "\n",
    "# As tokens em comum entre ambos os textos são de palavras muito frequentes como pronomes e preposições (she, a, it's)\n",
    "# Os textos escolhidos são o primeiro e o último episódio e uma série. Enquanto as tokens comuns mostram o \n",
    "# nome da protagonista (Korra) os tokens exclusivos mostram personagens que aparecem apenas naquele episódio. \n",
    "# Interessante notar que algumas preposições, apesar de comuns, aparecem como exclusivas (an, me, who, around, them)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5qbVHNYDOClP"
   },
   "source": [
    "**Questão 3**  \n",
    "(a) Usando a função FreqDist do NLTK, obtenha as frequências absolutas de três palavras à sua escolha em cada texto/corpus.  \n",
    "(b) Calcule a frequência relativa das mesmas três palavras nos dois textos/corpora.  \n",
    "(c) Qual das duas medidas (frequência absoluta ou frequência relativa) é, geralmente, mais útil? Por quê? _[análise qualitativa]_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "DkhaSYSPRgFn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "01_Welcome_to_Republic_City\n",
      "\n",
      "Korra: 250\n",
      "camera: 34\n",
      "I: 85\n",
      "\n",
      "50_The_Last_Stand_\n",
      "\n",
      "Korra: 106\n",
      "camera: 4\n",
      "I: 88\n",
      "\n",
      "01_Welcome_to_Republic_City\n",
      "\n",
      "Korra: 0.0086\n",
      "camera: 0.0003\n",
      "I: 0.0071\n",
      "\n",
      "50_The_Last_Stand_\n",
      "\n",
      "Korra: 0.0133\n",
      "camera: 0.0005\n",
      "I: 0.0111\n"
     ]
    }
   ],
   "source": [
    "# Resolução da questão 3.\n",
    "# a\n",
    "# texto1\n",
    "words_to_freqs = ['Korra','camera','I']\n",
    "\n",
    "for file in corpus.fileids():\n",
    "    print('\\n'+file+'\\n')\n",
    "    freqs = FreqDist(corpus.words(file))\n",
    "    for word in words_to_freqs:\n",
    "        print(\"{0}: {1}\".format(word, str(freqs[word])))\n",
    "\n",
    "# b\n",
    "for file in corpus.fileids():\n",
    "    print('\\n'+file+'\\n')\n",
    "    for i in words_to_freqs:\n",
    "        print(\"{0}: {1:.4f}\".format(i,freqs[i]/len(corpus.words(file))))\n",
    "        \n",
    "##### Questão 3.c)\n",
    "\n",
    "# A frequência relativa é sempre mais útil pois o valor carrega dentro de si a comparação com o tamanho do texto. \n",
    "# Sabemos que a token \"Korra\" corresponte a 2% do primeiro arquivo e 3% do segundo arquivo, assim, já sabe-se \n",
    "# que a frequência é relativamente alta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hbx5RcRXf1CS"
   },
   "source": [
    "**Questão 4**  \n",
    "(a) Use o NLTK para obter possíveis colocações em cada um dos seus textos/corpora.  \n",
    "(b) Analise as saídas do item (a). O que elas dizem sobre seus textos/corpora? _[análise qualitativa]_  \n",
    "(c) Escolha uma palavra e use a função do NLTK para imprimir algumas linhas de concordância referentes a essa palavra em cada um dos textos/corpora.  \n",
    "(d) É possível dizer sobre algo sobre o uso dessa palavra nos seus textos/corpora a partir da análise dessas linhas de concordância? _[análise qualitativa]_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "Emo2Y5XX2S8T",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collocation para arquivo: 01_Welcome_to_Republic_City\n",
      "\n",
      "White Lotus; Republic City; bear dog; polar bear; camera pans; Lotus\n",
      "leader; aerial shot; Southern Water; Water Tribe; Scene changes; side\n",
      "shot; Temple Island; Air Temple; Lotus members; Toed Ping; Two Toed;\n",
      "frontal view; overview shot; Chief Beifong; back shot\n",
      "None\n",
      "\n",
      "\n",
      "collocation para arquivo: 50_The_Last_Stand_\n",
      "\n",
      "shot cuts; lava disc; wide shot; metal plates; dance floor; Spirit\n",
      "Wilds; Earth Kingdom; Republic City; spirit portal; mecha suit; vine\n",
      "core; Spirit World; mecha suits; scene cuts; sideways close; engine\n",
      "room; control panel; President Raiko; bumpy ride; lawfully wedded\n",
      "None\n",
      "\n",
      "\n",
      "concordancia para arquivo: 01_Welcome_to_Republic_City\n",
      "\n",
      "Displaying 25 of 39 matches:\n",
      "le , and she sends it flying at the camera which blocks the entire screen when\n",
      " to a picture of Team Avatar as the camera pans across .] When I was a boy , m\n",
      " United Republic of Nations , [ The camera zooms across the map .] a society w\n",
      " great land , Republic City . [ The camera zooms toward the city across the wa\n",
      "markable things in his life . [ The camera pans to show a large statue of Aang\n",
      " Aang on Aang Memorial Island . The camera pans up to the sky .] But sadly , h\n",
      "le of the Avatar began anew . [ The camera pans up to the sun , where the show\n",
      "f the sky while it is snowing . The camera pans down to reveal the dimly lit h\n",
      " a small puddle on the ground . The camera pans up to show the impressed and s\n",
      "ly , she bends a fire stream at the camera , blocking the entire camera ' s vi\n",
      "at the camera , blocking the entire camera ' s view of the scene . When the fi\n",
      "sly shoot fire streams at her . The camera moves back to reveal that five peop\n",
      "ividual , smiling confidently . The camera moves back to show how Korra runs s\n",
      ".] That a girl . Go ! Go ! Go ! The camera pans out , as the polar bear dog st\n",
      "we there yet ? Are we there yet ? [ Camera pans up to show the belly of the fl\n",
      "hands on her protruding belly . The camera moves back to show how Pema descend\n",
      " , as Katara keeps her hand on it . Camera pans above to show a smiling Pema .\n",
      "o wait . Fade to black , before the camera changes to an aerial shot of the So\n",
      "r is what he had in mind . Tenzin [ Camera zooms back to a shot of the three .\n",
      "someone is sitting at the rim . The camera slowly zooms in on it , and suddenl\n",
      "ch to a back shot of the duo as the camera zooms out , revealing that they are\n",
      "ree of them , Korra ' s back to the camera , as they break up the hug . Her pa\n",
      "ut to a Satomobile in the tonnage . Camera pans to the right , revealing Naga \n",
      "aga , we ' re here ! Korra runs off camera . Scene switches to the ship ' s me\n",
      "o airships are floating above . The camera pans to the left , revealing the en\n",
      "None\n",
      "\n",
      "\n",
      "concordancia para arquivo: 50_The_Last_Stand_\n",
      "\n",
      "Displaying 4 of 4 matches:\n",
      "too . Now go ! Bolin runs off . The camera pans up the spirit vine core again \n",
      " ; in a loud whisper .] Hey , pal , camera on me ! This is my big day ! [ The \n",
      " on me ! This is my big day ! [ The camera is swiveled around and Varrick nods\n",
      "zing into each other ' s eyes . The camera pans up into the portal ' s beam of\n",
      "None\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Resolução da questão 4.\n",
    "# a\n",
    "\n",
    "\n",
    "for file in corpus.fileids():\n",
    "    print('collocation para arquivo: {0}\\n'.format(file))\n",
    "    print(nltk.Text(corpus.words(file)).collocations())\n",
    "    print('\\n')\n",
    "    \n",
    "#### b\n",
    "\n",
    "# As colocações conseguem mostrar expressões que são semânticamente relevantes para o enredo da série, \n",
    "# como também mostra expressões típicas de scripts (frontal view, overview shot), que descrevem a cena.\n",
    "\n",
    "\n",
    "# 4.c)\n",
    "for file in corpus.fileids():\n",
    "    print('concordancia para arquivo: {0}\\n'.format(file))\n",
    "    print(nltk.Text(corpus.words(file)).concordance('camera'))\n",
    "    print('\\n')\n",
    "    \n",
    "##### 4. d) #########################\n",
    "\n",
    "#  Ao pesquisar a palavra \"camera\" é possível perceber que ela é quase exclusiva de uso para descrever a cena. \n",
    "# Vemos expressões como \"camera zooms\", \"camera pans\" e \"camera changes\" em ambos os textos. \n",
    "# O único uso da palavra \"camera\" por um personagem ocorre no segundo texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3qkj687Ia0he"
   },
   "source": [
    "**Questão 5**  \n",
    "(a) Promova uma limpeza nos seus textos/corpora: gere listas mantendo apenas as palavras que contenham exclusivamente caracteres alfabéticos (isto é: números, sinais de pontuação etc. não devem ser considerados) e padronizando a capitalização (isto é, transformando todos os caracteres maiúsculos em minúsculos). Conte o número de types e de tokens após a limpeza.  \n",
    "(b) Compare os números de types e de tokens nos seus textos/corpora antes e após a limpeza realizada em (a). O que explica a diferença nesses valores? _[análise qualitativa]_  \n",
    "(c) Por que pode (ou não) ser útil realizar uma limpeza desse tipo? Cite uma ocasião em que essa limpeza é útil e outra ocasião em que ela é prejudicial para a análise linguística a ser executada. _[análise qualitativa]_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "8yt9xNd7xfYr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "01_Welcome_to_Republic_City\n",
      "\n",
      "Não limpo\n",
      "\n",
      "Número de Tokens: 12308\n",
      "Número de types: 2049\n",
      "\n",
      "\n",
      "50_The_Last_Stand_\n",
      "\n",
      "Não limpo\n",
      "\n",
      "Número de Tokens: 7961\n",
      "Número de types: 1635\n",
      "\n",
      "\n",
      "01_Welcome_to_Republic_City\n",
      "\n",
      "limpo\n",
      "\n",
      "Número de Tokens: 10215\n",
      "Número de types: 1824\n",
      "\n",
      "\n",
      "50_The_Last_Stand_\n",
      "\n",
      "limpo\n",
      "\n",
      "Número de Tokens: 6732\n",
      "Número de types: 1493\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Resolução da questão 5.\n",
    "    \n",
    "processados = []\n",
    "for file in corpus.fileids():\n",
    "    processados.append([token.lower() for token in corpus.words(file) if token.isalpha()])\n",
    "      \n",
    "for file in corpus.fileids():\n",
    "    words = corpus.words(file)\n",
    "    print('\\n'+file+'\\n')\n",
    "    print('Não limpo\\n')\n",
    "    print('Número de Tokens: {0}\\nNúmero de types: {1}\\n'.format(len(words),len(set(words))))\n",
    "\n",
    "for file in corpus.fileids():\n",
    "    print('\\n'+file+'\\n')\n",
    "    print('limpo\\n')\n",
    "    words = [token.lower() for token in corpus.words(file) if token.isalpha()]\n",
    "    print('Número de Tokens: {0}\\nNúmero de types: {1}\\n'.format(len(words),len(set(words))))\n",
    "    \n",
    "######### 5. b) #############\n",
    "# A diminuição no número de tokens se dá pela retirada de pontuação que são considerados como os próprios tokens\n",
    "# pelo NLTK. Já a diminuição no número de types se dá não só pela retirada da pontuação, como também pelo fim da\n",
    "# diferenciação entre palavras maiúsculas e minúsculas. Dessa forma palavras como \"Antes\" e \"antes\" agora são\n",
    "# um type só.\n",
    "\n",
    "########## 5. c) ############\n",
    "# Uma limpeza desse tipo é útil, por exemplo, na criação de modelos de Word Embeddings, onde elementos como pontuação\n",
    "# e palavras de função exclusivamente gramatical não são relevantes para uma análise semântica. Entretanto, essa mesma\n",
    "# limpeza pode ser prejudicial caso se deseje realizar uma marcação ou etiquetação do corpus, pois modelos computacionais\n",
    "# que realizam essa tarefa levam em consideração a pontuação e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9SASyJgquTYH"
   },
   "source": [
    "**Questão 6**  \n",
    "(a) Imprima, em ordem alfabética e no arquivo '/content/questao5.txt', todas as palavras com uma determinada característica, à sua escolha, em cada um dos textos/corpora, após a limpeza promovida na questão anterior. Essa característica deve ser uma determinada terminação (um possível sufixo) ou um início de palavra específico (um possível prefixo).  \n",
    "(b) Analise as listas de palavras obtidas. Até que ponto é possível usar esse método para a identificação de prefixos/sufixos em um texto/corpus? Quais são as vantagens e as limitações dessa abordagem? _[análise qualitativa]_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "HFFTOP7Y18gB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "abruptly\n",
      "absolutely\n",
      "aggressively\n",
      "angrily\n",
      "apologetically\n",
      "arrogantly\n",
      "awkwardly\n",
      "belly\n",
      "blatantly\n",
      "briefly\n",
      "broadly\n",
      "calmly\n",
      "certainly\n",
      "clearly\n",
      "cockily\n",
      "completely\n",
      "comprehensively\n",
      "confidently\n",
      "contently\n",
      "culpably\n",
      "decidedly\n",
      "defensively\n",
      "defiantly\n",
      "definitely\n",
      "diddly\n",
      "dimly\n",
      "early\n",
      "easily\n",
      "effectively\n",
      "effortlessly\n",
      "elderly\n",
      "enthusiastically\n",
      "exactly\n",
      "exaggeratedly\n",
      "excitedly\n",
      "family\n",
      "ferociously\n",
      "finally\n",
      "fly\n",
      "frantically\n",
      "furiously\n",
      "gently\n",
      "gleefully\n",
      "guiltily\n",
      "happily\n",
      "harshly\n",
      "haughtily\n",
      "heavily\n",
      "heroically\n",
      "honestly\n",
      "horizontally\n",
      "immediately\n",
      "innocently\n",
      "inquiringly\n",
      "irritably\n",
      "joyously\n",
      "lightly\n",
      "loudly\n",
      "mockingly\n",
      "narrowly\n",
      "nearly\n",
      "nimbly\n",
      "nonchalantly\n",
      "obviously\n",
      "only\n",
      "presently\n",
      "proudly\n",
      "provokingly\n",
      "questioningly\n",
      "quickly\n",
      "rapidly\n",
      "really\n",
      "respectfully\n",
      "richly\n",
      "sadly\n",
      "sarcastically\n",
      "seriously\n",
      "simultaneously\n",
      "slightly\n",
      "slowly\n",
      "slyly\n",
      "softly\n",
      "solemnly\n",
      "sorrowfully\n",
      "sternly\n",
      "subsequently\n",
      "successfully\n",
      "suddenly\n",
      "threateningly\n",
      "tightly\n",
      "totally\n",
      "uncertainly\n",
      "warmly\n",
      "\n",
      "\n",
      "abruptly\n",
      "actually\n",
      "angrily\n",
      "badly\n",
      "briefly\n",
      "calmly\n",
      "closely\n",
      "completely\n",
      "comply\n",
      "desperately\n",
      "directly\n",
      "easily\n",
      "emotionally\n",
      "exactly\n",
      "excitedly\n",
      "exponentially\n",
      "family\n",
      "finally\n",
      "firmly\n",
      "fly\n",
      "freely\n",
      "fully\n",
      "gradually\n",
      "happily\n",
      "honestly\n",
      "immediately\n",
      "lawfully\n",
      "lazily\n",
      "loudly\n",
      "lovingly\n",
      "momentarily\n",
      "nearly\n",
      "nervously\n",
      "only\n",
      "physically\n",
      "promptly\n",
      "quickly\n",
      "really\n",
      "respectively\n",
      "sadly\n",
      "simultaneously\n",
      "slowly\n",
      "softly\n",
      "suddenly\n",
      "utterly\n",
      "weekly\n",
      "wildly\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Resolução da questão 6.\n",
    "\n",
    "######## 6. a)  #############\n",
    "\n",
    "# Foi escolhido o sufixo -ly do inglês que é característico de advérbios como em \"happily\", \"easily\"\n",
    "\n",
    "with open('content/questao5.txt', 'w') as questao:\n",
    "    for texto in processados:\n",
    "        questao.write('\\n\\n')\n",
    "        conjunto = sorted(set(texto))\n",
    "        for token in conjunto:\n",
    "            if token[-2:] == 'ly':\n",
    "                questao.write(token+'\\n')\n",
    "\n",
    "with open('content/questao5.txt', 'r') as resposta:\n",
    "    print(resposta.read())\n",
    "    \n",
    "# Apesar de o método encontrar todos os adverbios, outras palavras, pertencentes a outra classe, mas que possuem \n",
    "# a mesma terminação também foram impressas. Algumas selas são \"family\", \"fly\", \"belly\". Dessa forma a análise\n",
    "# fica limitada a morfologia da sentença."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "prova-teorico-pratica - Lucas Fonseca Lage",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
